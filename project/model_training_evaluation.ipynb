Flight Price Prediction Notebook
# =======================================================
# Step 1: Importing Required Libraries
# =======================================================
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')  # Suppress warnings for cleaner output

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from xgboost import XGBRegressor
# =======================================================
# Step 2: Loading the Dataset
# =======================================================
train_df = pd.read_csv("/kaggle/input/mlp-term-2-2025-kaggle-assignment-1/train.csv")
test_df = pd.read_csv("/kaggle/input/mlp-term-2-2025-kaggle-assignment-1/test.csv")
submission_df = pd.read_csv("/kaggle/input/mlp-term-2-2025-kaggle-assignment-1/sample_submission.csv")

test_original = test_df.copy()
# =======================================================
# Step 3: Checking Data Types of Each Column
# =======================================================
print(train_df.dtypes)
id               int64
airline         object
flight          object
source          object
departure       object
stops           object
arrival         object
destination     object
class           object
duration       float64
days_left      float64
price            int64
dtype: object
# =======================================================
# Step 4: Descriptive Statistics for Numerical Features
# =======================================================
print(train_df.describe())
print(train_df[['duration', 'days_left', 'price']].median())
                id      duration     days_left         price
count  40000.00000  36987.000000  35562.000000   40000.00000
mean   19999.50000     12.004088     26.197936   20801.49025
std    11547.14972      7.108063     13.469232   22729.14842
min        0.00000      0.830000      1.000000    1105.00000
25%     9999.75000      6.670000     15.000000    4687.00000
50%    19999.50000     11.080000     26.000000    7353.00000
75%    29999.25000     15.920000     38.000000   42521.00000
max    39999.00000     47.080000     49.000000  114704.00000
duration       11.08
days_left      26.00
price        7353.00
dtype: float64
# =======================================================
# Step 5: Handling Missing Values
# =======================================================
print(train_df.isnull().sum())
train_df = train_df.dropna()
test_df = test_df.fillna(method='ffill')
id                0
airline        4613
flight            0
source            0
departure      4792
stops          2319
arrival           0
destination       0
class             0
duration       3013
days_left      4438
price             0
dtype: int64
# =======================================================
# Step 6: Identifying and Removing Duplicate Rows
# =======================================================
print("Duplicates in train:", train_df.duplicated().sum())
train_df = train_df.drop_duplicates()
Duplicates in train: 0
# =======================================================
# Step 7: Outlier Detection and Handling
# =======================================================
for col in ['duration', 'days_left', 'price']:
    sns.boxplot(x=train_df[col])
    plt.title(f"Boxplot of {col}")
    plt.show()

Q1 = train_df['price'].quantile(0.25)
Q3 = train_df['price'].quantile(0.75)
IQR = Q3 - Q1
train_df = train_df[(train_df['price'] >= Q1 - 1.5 * IQR) & (train_df['price'] <= Q3 + 1.5 * IQR)]



# =======================================================
# Step 8: Data Visualization and Insights
# =======================================================
sns.countplot(data=train_df, x='airline', order=train_df['airline'].value_counts().index)
plt.xticks(rotation=90)
plt.title("Number of Flights per Airline")
plt.show()

sns.boxplot(data=train_df, x='airline', y='price')
plt.xticks(rotation=90)
plt.title("Distribution of Ticket Prices by Airline")
plt.show()

sns.scatterplot(data=train_df, x='days_left', y='price')
plt.title("Ticket Price vs Days Left to Departure")
plt.show()



# =======================================================
# Step 9: Encoding Categorical Features and Scaling Numerical Features
# =======================================================
categorical_cols = ['airline', 'flight', 'source', 'departure', 'stops', 'arrival', 'destination', 'class']

# Combine train and test sets to ensure consistent encoding
combined = pd.concat([train_df[categorical_cols], test_df[categorical_cols]], axis=0)
for col in categorical_cols:
    le = LabelEncoder()
    combined[col] = le.fit_transform(combined[col])
    train_df[col] = combined[:len(train_df)][col].values
    test_df[col] = combined[len(train_df):][col].values

# Standardize numerical columns
scaler = StandardScaler()
train_df[['duration', 'days_left']] = scaler.fit_transform(train_df[['duration', 'days_left']])
test_df[['duration', 'days_left']] = scaler.transform(test_df[['duration', 'days_left']])
# =======================================================
# Step 10: Model Training and Evaluation
# =======================================================
X = train_df.drop(['price'], axis=1)
y = train_df['price']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "LinearRegression": LinearRegression(),
    "Ridge": Ridge(),
    "Lasso": Lasso(),
    "DecisionTree": DecisionTreeRegressor(),
    "RandomForest": RandomForestRegressor(),
    "XGBoost": XGBRegressor(),
    "KNN": KNeighborsRegressor(),
    "GradientBoosting": GradientBoostingRegressor()
}

results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_val)
    mae = mean_absolute_error(y_val, preds)
    rmse = np.sqrt(mean_squared_error(y_val, preds))
    r2 = r2_score(y_val, preds)
    results.append((name, mae, rmse, r2))

results_df = pd.DataFrame(results, columns=['Model', 'MAE', 'RMSE', 'R2'])
print("Model Performance Summary:")
print(results_df.sort_values(by='RMSE'))
Model Performance Summary:
              Model           MAE          RMSE        R2
4      RandomForest   1649.012261   3184.580968  0.980386
5           XGBoost   1951.644563   3411.976696  0.977485
7  GradientBoosting   2750.392299   4590.807078  0.959240
3      DecisionTree   1991.082141   4643.889203  0.958292
0  LinearRegression   4559.289436   6885.082733  0.908320
2             Lasso   4559.064111   6885.095937  0.908320
1             Ridge   4559.912485   6885.100737  0.908319
6               KNN  16900.783116  22288.246705  0.039255
# =======================================================
# Step 11: Hyperparameter Tuning for Selected Models
# =======================================================
# Random Forest
rf_params = {'n_estimators': [100, 200], 'max_depth': [10, 20]}
gs_rf = GridSearchCV(RandomForestRegressor(), rf_params, cv=3)
gs_rf.fit(X_train, y_train)
print("Best Random Forest Parameters:", gs_rf.best_params_)

# Ridge Regression
ridge_params = {'alpha': [0.1, 1, 10]}
gs_ridge = GridSearchCV(Ridge(), ridge_params, cv=3)
gs_ridge.fit(X_train, y_train)
print("Best Ridge Parameters:", gs_ridge.best_params_)

# XGBoost
xgb_params = {'n_estimators': [100, 200], 'max_depth': [3, 5]}
gs_xgb = GridSearchCV(XGBRegressor(), xgb_params, cv=3)
gs_xgb.fit(X_train, y_train)
print("Best XGBoost Parameters:", gs_xgb.best_params_)
Best Random Forest Parameters: {'max_depth': 20, 'n_estimators': 200}
Best Ridge Parameters: {'alpha': 0.1}
Best XGBoost Parameters: {'max_depth': 5, 'n_estimators': 200}
# =======================================================
# Step 12: Final Prediction and Submission File
# =======================================================
final_model = gs_xgb.best_estimator_
final_model.fit(X, y)
final_preds = final_model.predict(test_df)

submission_df['price'] = final_preds
submission_df.to_csv("submission.csv", index=False)
print("Submission file generated successfully.")
Submission file generated successfully.
